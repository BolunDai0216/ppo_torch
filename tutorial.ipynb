{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial For ppo_torch\n",
    "\n",
    "In this notebook, I will go through how PPO works and how it is implemented in the `ppo_torch` library. This notebook does not contain any code for training, in the hope that the training loop is self-explanitory. Here is a list of what this notebook will go through:\n",
    "\n",
    "- [Creating multi-layer perceptrons](#mlp)\n",
    "- [Creating Actors](#actors_create)\n",
    "- [Updating Actors](#actors_update)\n",
    "- [Creating Critics](#critics_create)\n",
    "- [Updating Critics](#critics_update)\n",
    "- [Replay Buffer](#replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from actor import ActorContinuous, ActorDiscrete\n",
    "from critic import Critic\n",
    "from mlp import mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Multi-Layer Perceptrons\n",
    "<a id='mlp'></a>\n",
    "\n",
    "The function that creates a multi-layer perceptron, `mlp()`, is defined in `mlp.py`. It takes in two lists\n",
    "\n",
    "- `feature_sizes`: a list of the size of each layer of the mlp\n",
    "- `activation`: a list of the activation functions for each layer of the mlp\n",
    "\n",
    "For example, if the MLP has input size of 8, output size of 4 and 2 hidden layers of size 16, we will define `nn_size = [8, 16, 16, 4]`. Since there are three layers:\n",
    "\n",
    "- Input Layer (8, 16)\n",
    "- Hidden Layer (16, 16)\n",
    "- Output Layer (16, 4)\n",
    "\n",
    "and we set all layers to have ReLU activation, we can define `activations = [nn.ReLU, nn.ReLU, nn.ReLU]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (5): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nn_sizes = [8, 16, 16, 4]\n",
    "activations = [nn.ReLU, nn.ReLU, nn.ReLU]\n",
    "mlp_model = mlp(nn_sizes, activations)\n",
    "print(mlp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Actors\n",
    "<a id='actors_create'></a>\n",
    "\n",
    "We can set that the `obs_dim = 8` and `act_dim = 4`, we also set the hidden layer have both the `in_feature` and `out_feature` to be 16. Thus, we have `hidden_dim = [16, 16]`. Additionally, we use ReLU for all of the activations, then we have `activations = [nn.ReLU, nn.ReLU, nn.ReLU]`.\n",
    "\n",
    "Using this set up, we can define actors for both continuous and discrete action spaces. First, we define a discrete actor using the `ActorDiscrete` class, which is defined in `actor.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 8\n",
    "act_dim = 4\n",
    "hidden_dim = [16, 16]\n",
    "activations = [nn.ReLU, nn.ReLU, nn.ReLU]\n",
    "\n",
    "actor_d = ActorDiscrete(obs_dim, act_dim, hidden_dim, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (5): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(actor_d.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the neural network is defined as `actor_d.net`. Next, we see what is the output of `actor_d.forward(obs)`, note this can be directly called using `actor_d(obs)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical(logits: torch.Size([1, 4]))\n",
      "tensor([2])\n"
     ]
    }
   ],
   "source": [
    "obs = torch.ones(1, 8)\n",
    "pi, _ = actor_d(obs)\n",
    "print(pi)\n",
    "print(pi.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output is not the action, but a probability distribution. Here for discrete actions, the output of `actor_d` is a `Categorical` distribution, we can sample from this distribution `pi` using `pi.sample()`. The output of this sampling is the index of the sampled action, here the output will only be 0, 1, 2, 3.\n",
    "\n",
    "Next we do the same thing for the actor for continuous action spaces using the `ActorContinuous` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 8\n",
    "act_dim = 4\n",
    "hidden_dim = [16, 16]\n",
    "activations = [nn.ReLU, nn.ReLU, nn.ReLU]\n",
    "\n",
    "actor_c = ActorContinuous(obs_dim, act_dim, hidden_dim, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (5): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(actor_c.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultivariateNormal(loc: torch.Size([1, 4]), covariance_matrix: torch.Size([1, 4, 4]))\n",
      "tensor([[-0.1981,  0.1785, -0.1717,  0.3097]])\n"
     ]
    }
   ],
   "source": [
    "obs = torch.ones(1, 8)\n",
    "pi, _ = actor_c(obs)\n",
    "print(pi)\n",
    "print(pi.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since here we have a continuous action space, the output of `actor_c.forward()` will be a normal distribution with mean `pi.loc`, which is the same as `actor_c.net(obs)` and a fixed covariance matrix `pi.covariance_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.0000, 0.0093, 0.3460]], grad_fn=<ExpandBackward>),\n",
       " tensor([[0.0000, 0.0000, 0.0093, 0.3460]], grad_fn=<ReluBackward0>),\n",
       " tensor([[[0.0400, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0400, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0400, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0400]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.loc, actor_c.net(obs), pi.covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Actors\n",
    "<a id='actors_update'></a>\n",
    "\n",
    "### Actors Loss Function\n",
    "The actor is updated by maximizing the PPO-Clip objective\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
    "\\theta_{k+1} = \\argmax_\\theta\\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau\\in\\mathcal{D}_k}\\sum_{t=0}^T\\min\\Bigg(\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_k}(a_t\\mid s_t)}A^{\\pi_{\\theta_k}}(a_t, s_t),\\ g\\Big(\\epsilon, A^{\\pi_{\\theta_k}}(a_t, s_t)\\Big)\\Bigg)$$\n",
    "\n",
    "This seems like a mouthful of symbols, let's dissect it so it makes sense. Here $|\\mathcal{D}_k|$ represents the number of episodes used in optimizing the actor's weights, and $T$ is the number of step sizes for each episode. Thus, we can see the entire equation is simply saying the next weights $\\theta_k$ is the set of weights that has the largest mean\n",
    "\n",
    "$$\\min\\Bigg(\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_k}(a_t\\mid s_t)}A^{\\pi_{\\theta_k}}(a_t, s_t),\\ g\\Big(\\epsilon, A^{\\pi_{\\theta_k}}(a_t, s_t)\\Big)\\Bigg).$$\n",
    "\n",
    "We can see that the policy gradient loss\n",
    "\n",
    "$$\\mathcal{L}_{PG} = \\mathbb{E}_t\\Bigg[\\log\\Big(\\pi_\\theta(a_t\\mid s_t)\\Big)A^{\\pi_{\\theta_k}}(a_t, s_t)\\Bigg]$$\n",
    "\n",
    "has the same gradient w.r.t $\\theta$ as\n",
    "\n",
    "$$\\bar{\\mathcal{L}}_{PPO} = \\mathbb{E}_t\\Bigg[\\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_k}(a_t\\mid s_t)}A^{\\pi_{\\theta_k}}(a_t, s_t)\\Bigg].$$\n",
    "\n",
    "Note $A^{\\pi_{\\theta_k}}(a_t, s_t)$, which is the generalized advantage estimation, is not a function of $\\theta$, we will go into this later. We have\n",
    "\n",
    "$$\\nabla_\\theta\\log\\Big(\\pi_\\theta(a_t\\mid s_t)\\Big)\\Big|_{\\theta_k} = \\Bigg(\\frac{\\partial\\log\\Big(\\pi_\\theta(a_t\\mid s_t)\\Big)}{\\partial\\pi_\\theta(a_t\\mid s_t)}\\frac{\\partial\\pi_\\theta(a_t\\mid s_t)}{\\partial\\theta}\\Bigg)\\Bigg|_{\\theta_k} = \\frac{\\nabla_\\theta\\pi_\\theta(a_t\\mid s_t)}{\\pi_\\theta(a_t\\mid s_t)}\\Big|_{\\theta_k} = \\nabla_\\theta\\Big(\\frac{\\nabla_\\theta\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_k}(a_t\\mid s_t)}\\Big)\\Big|_{\\theta_k}$$\n",
    "\n",
    "The function $g(\\epsilon, A)$ is defined as\n",
    "\n",
    "$$g(\\epsilon, A) = \\begin{cases}\n",
    "    (1+\\epsilon)A & A \\geq 0\\\\\n",
    "    (1-\\epsilon)A & A < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "The next policy will want to maximize the probability of state-action pairs with positive advantage and minimize the probability of state-action pairs with negative advantage. However, in both cases, PPO would not want to change the policy too much, hence using $g(\\epsilon, A)$ to regulate this amount of change.\n",
    "\n",
    "The loss function is defined in `ppo.py`, see the function `actor_loss()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Critics\n",
    "<a id='critics_create'></a>\n",
    "\n",
    "We can set the `obs_dim = 8`, since the output is the value function, it always has dimension 1. Similar to the actor, we set the `in_feature`'s and `out_feature`'s to be `hidden_dim = [16, 16]` and all activations to be `activations = [nn.ReLU, nn.ReLU, nn.ReLU]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = 8\n",
    "hidden_dim = [16, 16]\n",
    "activations = [nn.ReLU, nn.ReLU, nn.ReLU]\n",
    "critic = Critic(obs_dim, hidden_dim, activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the output of the critic is simply a one dimension value. For both the continuous and discrete actor, the structure of their critic would be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "obs = torch.ones(1, 8)\n",
    "val = critic(obs)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating Critics\n",
    "<a id='critics_create'></a>\n",
    "\n",
    "The critic is simply updated using the MSE loss between the value function estimation at each state $V_\\phi(s_t)$ and its reward-to-go $\\hat{R}(s_t)$ for one specific episode\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmin}{\\arg\\!\\min}\n",
    "\\mathcal{L}_{VF} = \\argmin_{\\phi}\\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau\\in\\mathcal{D}_k}\\sum_{t=0}^T\\Big(V_\\phi(s_t) - \\hat{R}(s_t)\\Big)\n",
    "$$\n",
    "\n",
    "The loss function is defined in `ppo.py`, see the function `critic_loss()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "<a id='replay_buffer'></a>\n",
    "\n",
    "The purpose of the replay buffer is to store the experiences for each episode. Instead of sampling a batch of data as in off-policy RL algorithms, PPO will use all of the data within the replay buffer to calculate a gradient estimation.\n",
    "\n",
    "One of the calculations that are done in the replay buffer is obtaining the generalized advantage estimation, which is explained below.\n",
    "\n",
    "### Generalized Advantage Estimation\n",
    "The one step TD error at time step $t$ is defined as\n",
    "\n",
    "$$\\delta_t(s_t, a_t, s_{t+1}) = r_t + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t)$$\n",
    "\n",
    "here the reward is a function of the current state $s_t$, the current action $a_t$ and the next state $s_{t+1}$. The advantage function is the expected TD error, where is taken w.r.t the system dynamics ([here is a nice article](http://boris-belousov.net/2017/08/10/td-advantage-bellman/))\n",
    "\n",
    "$$A^\\pi(s_t, a_t) = \\mathbb{E}_{s_{t+1}\\sim\\mathcal{P}}\\Big[r_t + \\gamma V^\\pi(s_{t+1})\\Big] - V^\\pi(s_t) = \\mathbb{E}_{s_{t+1}\\sim\\mathcal{P}}[\\delta_t].$$\n",
    "\n",
    "And the generalized advantage estimation (GAE) is simply\n",
    "\n",
    "$$A_{\\gamma, \\lambda}^\\pi(s_t, a_t) = \\sum_{l=0}^{H}(\\gamma\\lambda)^l\\delta_{t+l}$$\n",
    "\n",
    "where $H$ is the number of steps till the terminal step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl_env3",
   "language": "python",
   "name": "deeprl_env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
